{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# pip install requests_html\n",
        "!pip install pymongo requests-html newspaper3k transformers nest_asyncio regex torch sentencepiece\n",
        "!pip install  lxml_html_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTpD4eSSRX6p",
        "outputId": "0c0f71b8-6e86-47ad-ec3a-b64bfd06e0c3"
      },
      "id": "VTpD4eSSRX6p",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymongo\n",
            "  Downloading pymongo-4.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting requests-html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from requests-html) (2.32.3)\n",
            "Collecting pyquery (from requests-html)\n",
            "  Downloading pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting fake-useragent (from requests-html)\n",
            "  Downloading fake_useragent-2.0.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting parse (from requests-html)\n",
            "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting bs4 (from requests-html)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Collecting w3lib (from requests-html)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
            "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (4.13.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.3.1)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (3.9.1)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (2025.1.31)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.6.1)\n",
            "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->requests-html) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->requests-html) (3.10)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.21.0)\n",
            "Downloading pymongo-4.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading fake_useragent-2.0.3-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.1/201.1 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
            "Downloading pyquery-2.0.1-py3-none-any.whl (22 kB)\n",
            "Downloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading pyee-11.1.1-py3-none-any.whl (15 kB)\n",
            "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13539 sha256=59275a02c1b0fb6beb7a54b1a490077919a4422390041c49bfb5195e9f4f3965\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/f8/cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3342 sha256=3e0365d33d54f22a7df925e07e653ae74662b85bd4126f220a1afbf2b4901ea3\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d5/72/9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398379 sha256=20c07eb5a6cb499abbc335f33203455b40d19b9de6ef6eb3d2c254ad86738824\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/a1/46/8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=b94bcbb5e36d28460c019df7dc578daf5ca782c661a195611a046f5976ee9b16\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, parse, jieba3k, appdirs, websockets, w3lib, urllib3, pyee, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, feedparser, fake-useragent, dnspython, cssselect, pyquery, pyppeteer, pymongo, nvidia-cusparse-cu12, nvidia-cudnn-cu12, bs4, requests-html, requests-file, nvidia-cusolver-cu12, feedfinder2, tldextract, newspaper3k\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 14.2\n",
            "    Uninstalling websockets-14.2:\n",
            "      Successfully uninstalled websockets-14.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 0.8.0 requires websockets<15.0dev,>=13.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 bs4-0.0.2 cssselect-1.2.0 dnspython-2.7.0 fake-useragent-2.0.3 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 parse-1.20.2 pyee-11.1.1 pymongo-4.11.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-file-2.1.0 requests-html-0.10.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.1.3 urllib3-1.26.20 w3lib-2.3.1 websockets-10.4\n",
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from lxml_html_clean) (5.3.1)\n",
            "Downloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml_html_clean\n",
            "Successfully installed lxml_html_clean-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pyppeteer\n",
        "!pip install playwright\n",
        "!playwright install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq7fMIzWaMEp",
        "outputId": "03a722e3-1250-48df-9d5d-0032d5f5a7b0"
      },
      "id": "iq7fMIzWaMEp",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting playwright\n",
            "  Downloading playwright-1.50.0-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting pyee<13,>=12 (from playwright)\n",
            "  Downloading pyee-12.1.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<13,>=12->playwright) (4.12.2)\n",
            "Downloading playwright-1.50.0-py3-none-manylinux1_x86_64.whl (45.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-12.1.1-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyee, playwright\n",
            "  Attempting uninstall: pyee\n",
            "    Found existing installation: pyee 11.1.1\n",
            "    Uninstalling pyee-11.1.1:\n",
            "      Successfully uninstalled pyee-11.1.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pyppeteer 2.0.0 requires pyee<12.0.0,>=11.0.0, but you have pyee 12.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed playwright-1.50.0 pyee-12.1.1\n",
            "Downloading Chromium 133.0.6943.16 (playwright build v1155)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1155/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G163.5 MiB [] 0% 0.0s\u001b[0K\u001b[1G163.5 MiB [] 0% 53.8s\u001b[0K\u001b[1G163.5 MiB [] 0% 50.0s\u001b[0K\u001b[1G163.5 MiB [] 0% 36.9s\u001b[0K\u001b[1G163.5 MiB [] 0% 23.5s\u001b[0K\u001b[1G163.5 MiB [] 0% 15.4s\u001b[0K\u001b[1G163.5 MiB [] 1% 9.5s\u001b[0K\u001b[1G163.5 MiB [] 1% 6.3s\u001b[0K\u001b[1G163.5 MiB [] 2% 5.1s\u001b[0K\u001b[1G163.5 MiB [] 3% 4.0s\u001b[0K\u001b[1G163.5 MiB [] 4% 3.4s\u001b[0K\u001b[1G163.5 MiB [] 5% 3.1s\u001b[0K\u001b[1G163.5 MiB [] 6% 3.1s\u001b[0K\u001b[1G163.5 MiB [] 6% 3.0s\u001b[0K\u001b[1G163.5 MiB [] 7% 2.9s\u001b[0K\u001b[1G163.5 MiB [] 8% 2.7s\u001b[0K\u001b[1G163.5 MiB [] 9% 2.5s\u001b[0K\u001b[1G163.5 MiB [] 10% 2.5s\u001b[0K\u001b[1G163.5 MiB [] 12% 2.3s\u001b[0K\u001b[1G163.5 MiB [] 13% 2.2s\u001b[0K\u001b[1G163.5 MiB [] 13% 2.1s\u001b[0K\u001b[1G163.5 MiB [] 14% 2.1s\u001b[0K\u001b[1G163.5 MiB [] 15% 2.0s\u001b[0K\u001b[1G163.5 MiB [] 16% 2.0s\u001b[0K\u001b[1G163.5 MiB [] 17% 1.9s\u001b[0K\u001b[1G163.5 MiB [] 19% 1.8s\u001b[0K\u001b[1G163.5 MiB [] 20% 1.7s\u001b[0K\u001b[1G163.5 MiB [] 21% 1.7s\u001b[0K\u001b[1G163.5 MiB [] 22% 1.7s\u001b[0K\u001b[1G163.5 MiB [] 24% 1.6s\u001b[0K\u001b[1G163.5 MiB [] 25% 1.5s\u001b[0K\u001b[1G163.5 MiB [] 26% 1.5s\u001b[0K\u001b[1G163.5 MiB [] 27% 1.5s\u001b[0K\u001b[1G163.5 MiB [] 27% 1.6s\u001b[0K\u001b[1G163.5 MiB [] 28% 1.5s\u001b[0K\u001b[1G163.5 MiB [] 29% 1.5s\u001b[0K\u001b[1G163.5 MiB [] 30% 1.5s\u001b[0K\u001b[1G163.5 MiB [] 31% 1.4s\u001b[0K\u001b[1G163.5 MiB [] 32% 1.4s\u001b[0K\u001b[1G163.5 MiB [] 34% 1.3s\u001b[0K\u001b[1G163.5 MiB [] 35% 1.3s\u001b[0K\u001b[1G163.5 MiB [] 36% 1.3s\u001b[0K\u001b[1G163.5 MiB [] 37% 1.2s\u001b[0K\u001b[1G163.5 MiB [] 39% 1.2s\u001b[0K\u001b[1G163.5 MiB [] 40% 1.1s\u001b[0K\u001b[1G163.5 MiB [] 41% 1.1s\u001b[0K\u001b[1G163.5 MiB [] 42% 1.1s\u001b[0K\u001b[1G163.5 MiB [] 43% 1.1s\u001b[0K\u001b[1G163.5 MiB [] 44% 1.0s\u001b[0K\u001b[1G163.5 MiB [] 45% 1.0s\u001b[0K\u001b[1G163.5 MiB [] 47% 1.0s\u001b[0K\u001b[1G163.5 MiB [] 48% 1.0s\u001b[0K\u001b[1G163.5 MiB [] 49% 0.9s\u001b[0K\u001b[1G163.5 MiB [] 50% 0.9s\u001b[0K\u001b[1G163.5 MiB [] 52% 0.9s\u001b[0K\u001b[1G163.5 MiB [] 53% 0.8s\u001b[0K\u001b[1G163.5 MiB [] 54% 0.8s\u001b[0K\u001b[1G163.5 MiB [] 56% 0.8s\u001b[0K\u001b[1G163.5 MiB [] 57% 0.7s\u001b[0K\u001b[1G163.5 MiB [] 58% 0.7s\u001b[0K\u001b[1G163.5 MiB [] 60% 0.7s\u001b[0K\u001b[1G163.5 MiB [] 61% 0.7s\u001b[0K\u001b[1G163.5 MiB [] 62% 0.6s\u001b[0K\u001b[1G163.5 MiB [] 63% 0.6s\u001b[0K\u001b[1G163.5 MiB [] 65% 0.6s\u001b[0K\u001b[1G163.5 MiB [] 66% 0.6s\u001b[0K\u001b[1G163.5 MiB [] 67% 0.6s\u001b[0K\u001b[1G163.5 MiB [] 68% 0.5s\u001b[0K\u001b[1G163.5 MiB [] 69% 0.5s\u001b[0K\u001b[1G163.5 MiB [] 70% 0.5s\u001b[0K\u001b[1G163.5 MiB [] 71% 0.5s\u001b[0K\u001b[1G163.5 MiB [] 72% 0.5s\u001b[0K\u001b[1G163.5 MiB [] 73% 0.5s\u001b[0K\u001b[1G163.5 MiB [] 74% 0.4s\u001b[0K\u001b[1G163.5 MiB [] 75% 0.4s\u001b[0K\u001b[1G163.5 MiB [] 76% 0.4s\u001b[0K\u001b[1G163.5 MiB [] 77% 0.4s\u001b[0K\u001b[1G163.5 MiB [] 78% 0.4s\u001b[0K\u001b[1G163.5 MiB [] 79% 0.4s\u001b[0K\u001b[1G163.5 MiB [] 80% 0.3s\u001b[0K\u001b[1G163.5 MiB [] 81% 0.3s\u001b[0K\u001b[1G163.5 MiB [] 82% 0.3s\u001b[0K\u001b[1G163.5 MiB [] 83% 0.3s\u001b[0K\u001b[1G163.5 MiB [] 84% 0.3s\u001b[0K\u001b[1G163.5 MiB [] 86% 0.2s\u001b[0K\u001b[1G163.5 MiB [] 87% 0.2s\u001b[0K\u001b[1G163.5 MiB [] 88% 0.2s\u001b[0K\u001b[1G163.5 MiB [] 89% 0.2s\u001b[0K\u001b[1G163.5 MiB [] 91% 0.2s\u001b[0K\u001b[1G163.5 MiB [] 92% 0.1s\u001b[0K\u001b[1G163.5 MiB [] 93% 0.1s\u001b[0K\u001b[1G163.5 MiB [] 95% 0.1s\u001b[0K\u001b[1G163.5 MiB [] 96% 0.1s\u001b[0K\u001b[1G163.5 MiB [] 97% 0.0s\u001b[0K\u001b[1G163.5 MiB [] 99% 0.0s\u001b[0K\u001b[1G163.5 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 133.0.6943.16 (playwright build v1155) downloaded to /root/.cache/ms-playwright/chromium-1155\n",
            "Downloading Chromium Headless Shell 133.0.6943.16 (playwright build v1155)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1155/chromium-headless-shell-linux.zip\u001b[22m\n",
            "\u001b[1G99.9 MiB [] 0% 0.0s\u001b[0K\u001b[1G99.9 MiB [] 0% 43.8s\u001b[0K\u001b[1G99.9 MiB [] 0% 22.7s\u001b[0K\u001b[1G99.9 MiB [] 0% 13.8s\u001b[0K\u001b[1G99.9 MiB [] 0% 7.8s\u001b[0K\u001b[1G99.9 MiB [] 1% 5.1s\u001b[0K\u001b[1G99.9 MiB [] 3% 3.4s\u001b[0K\u001b[1G99.9 MiB [] 4% 2.8s\u001b[0K\u001b[1G99.9 MiB [] 5% 2.2s\u001b[0K\u001b[1G99.9 MiB [] 7% 1.9s\u001b[0K\u001b[1G99.9 MiB [] 8% 1.7s\u001b[0K\u001b[1G99.9 MiB [] 10% 1.8s\u001b[0K\u001b[1G99.9 MiB [] 11% 1.8s\u001b[0K\u001b[1G99.9 MiB [] 12% 1.8s\u001b[0K\u001b[1G99.9 MiB [] 13% 1.7s\u001b[0K\u001b[1G99.9 MiB [] 14% 1.7s\u001b[0K\u001b[1G99.9 MiB [] 15% 1.6s\u001b[0K\u001b[1G99.9 MiB [] 16% 1.6s\u001b[0K\u001b[1G99.9 MiB [] 17% 1.5s\u001b[0K\u001b[1G99.9 MiB [] 20% 1.4s\u001b[0K\u001b[1G99.9 MiB [] 22% 1.3s\u001b[0K\u001b[1G99.9 MiB [] 23% 1.2s\u001b[0K\u001b[1G99.9 MiB [] 24% 1.2s\u001b[0K\u001b[1G99.9 MiB [] 26% 1.1s\u001b[0K\u001b[1G99.9 MiB [] 28% 1.1s\u001b[0K\u001b[1G99.9 MiB [] 31% 1.0s\u001b[0K\u001b[1G99.9 MiB [] 32% 0.9s\u001b[0K\u001b[1G99.9 MiB [] 34% 0.9s\u001b[0K\u001b[1G99.9 MiB [] 35% 0.9s\u001b[0K\u001b[1G99.9 MiB [] 37% 0.8s\u001b[0K\u001b[1G99.9 MiB [] 39% 0.8s\u001b[0K\u001b[1G99.9 MiB [] 41% 0.8s\u001b[0K\u001b[1G99.9 MiB [] 43% 0.7s\u001b[0K\u001b[1G99.9 MiB [] 45% 0.7s\u001b[0K\u001b[1G99.9 MiB [] 46% 0.7s\u001b[0K\u001b[1G99.9 MiB [] 47% 0.7s\u001b[0K\u001b[1G99.9 MiB [] 49% 0.6s\u001b[0K\u001b[1G99.9 MiB [] 50% 0.6s\u001b[0K\u001b[1G99.9 MiB [] 51% 0.6s\u001b[0K\u001b[1G99.9 MiB [] 53% 0.6s\u001b[0K\u001b[1G99.9 MiB [] 55% 0.5s\u001b[0K\u001b[1G99.9 MiB [] 57% 0.5s\u001b[0K\u001b[1G99.9 MiB [] 59% 0.5s\u001b[0K\u001b[1G99.9 MiB [] 62% 0.4s\u001b[0K\u001b[1G99.9 MiB [] 65% 0.4s\u001b[0K\u001b[1G99.9 MiB [] 67% 0.4s\u001b[0K\u001b[1G99.9 MiB [] 70% 0.3s\u001b[0K\u001b[1G99.9 MiB [] 72% 0.3s\u001b[0K\u001b[1G99.9 MiB [] 74% 0.3s\u001b[0K\u001b[1G99.9 MiB [] 77% 0.2s\u001b[0K\u001b[1G99.9 MiB [] 79% 0.2s\u001b[0K\u001b[1G99.9 MiB [] 81% 0.2s\u001b[0K\u001b[1G99.9 MiB [] 83% 0.2s\u001b[0K\u001b[1G99.9 MiB [] 86% 0.1s\u001b[0K\u001b[1G99.9 MiB [] 88% 0.1s\u001b[0K\u001b[1G99.9 MiB [] 90% 0.1s\u001b[0K\u001b[1G99.9 MiB [] 93% 0.1s\u001b[0K\u001b[1G99.9 MiB [] 95% 0.0s\u001b[0K\u001b[1G99.9 MiB [] 98% 0.0s\u001b[0K\u001b[1G99.9 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 133.0.6943.16 (playwright build v1155) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1155\n",
            "Downloading Firefox 134.0 (playwright build v1471)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/firefox/1471/firefox-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G88.7 MiB [] 0% 0.0s\u001b[0K\u001b[1G88.7 MiB [] 0% 32.1s\u001b[0K\u001b[1G88.7 MiB [] 0% 26.7s\u001b[0K\u001b[1G88.7 MiB [] 0% 18.5s\u001b[0K\u001b[1G88.7 MiB [] 0% 13.6s\u001b[0K\u001b[1G88.7 MiB [] 1% 7.8s\u001b[0K\u001b[1G88.7 MiB [] 1% 5.1s\u001b[0K\u001b[1G88.7 MiB [] 3% 3.7s\u001b[0K\u001b[1G88.7 MiB [] 3% 3.4s\u001b[0K\u001b[1G88.7 MiB [] 4% 3.0s\u001b[0K\u001b[1G88.7 MiB [] 5% 2.7s\u001b[0K\u001b[1G88.7 MiB [] 7% 2.4s\u001b[0K\u001b[1G88.7 MiB [] 8% 2.2s\u001b[0K\u001b[1G88.7 MiB [] 9% 2.1s\u001b[0K\u001b[1G88.7 MiB [] 10% 2.0s\u001b[0K\u001b[1G88.7 MiB [] 11% 1.9s\u001b[0K\u001b[1G88.7 MiB [] 12% 2.0s\u001b[0K\u001b[1G88.7 MiB [] 13% 1.9s\u001b[0K\u001b[1G88.7 MiB [] 14% 1.8s\u001b[0K\u001b[1G88.7 MiB [] 15% 1.8s\u001b[0K\u001b[1G88.7 MiB [] 16% 1.7s\u001b[0K\u001b[1G88.7 MiB [] 17% 1.7s\u001b[0K\u001b[1G88.7 MiB [] 18% 1.7s\u001b[0K\u001b[1G88.7 MiB [] 19% 1.7s\u001b[0K\u001b[1G88.7 MiB [] 21% 1.6s\u001b[0K\u001b[1G88.7 MiB [] 22% 1.5s\u001b[0K\u001b[1G88.7 MiB [] 23% 1.4s\u001b[0K\u001b[1G88.7 MiB [] 25% 1.4s\u001b[0K\u001b[1G88.7 MiB [] 27% 1.4s\u001b[0K\u001b[1G88.7 MiB [] 28% 1.3s\u001b[0K\u001b[1G88.7 MiB [] 29% 1.3s\u001b[0K\u001b[1G88.7 MiB [] 30% 1.4s\u001b[0K\u001b[1G88.7 MiB [] 30% 1.5s\u001b[0K\u001b[1G88.7 MiB [] 30% 1.6s\u001b[0K\u001b[1G88.7 MiB [] 31% 1.6s\u001b[0K\u001b[1G88.7 MiB [] 33% 1.5s\u001b[0K\u001b[1G88.7 MiB [] 35% 1.4s\u001b[0K\u001b[1G88.7 MiB [] 36% 1.3s\u001b[0K\u001b[1G88.7 MiB [] 38% 1.3s\u001b[0K\u001b[1G88.7 MiB [] 39% 1.2s\u001b[0K\u001b[1G88.7 MiB [] 41% 1.2s\u001b[0K\u001b[1G88.7 MiB [] 42% 1.2s\u001b[0K\u001b[1G88.7 MiB [] 43% 1.1s\u001b[0K\u001b[1G88.7 MiB [] 44% 1.1s\u001b[0K\u001b[1G88.7 MiB [] 45% 1.1s\u001b[0K\u001b[1G88.7 MiB [] 46% 1.0s\u001b[0K\u001b[1G88.7 MiB [] 48% 1.0s\u001b[0K\u001b[1G88.7 MiB [] 50% 0.9s\u001b[0K\u001b[1G88.7 MiB [] 51% 0.9s\u001b[0K\u001b[1G88.7 MiB [] 52% 0.9s\u001b[0K\u001b[1G88.7 MiB [] 54% 0.9s\u001b[0K\u001b[1G88.7 MiB [] 56% 0.8s\u001b[0K\u001b[1G88.7 MiB [] 58% 0.8s\u001b[0K\u001b[1G88.7 MiB [] 59% 0.7s\u001b[0K\u001b[1G88.7 MiB [] 61% 0.7s\u001b[0K\u001b[1G88.7 MiB [] 62% 0.7s\u001b[0K\u001b[1G88.7 MiB [] 64% 0.6s\u001b[0K\u001b[1G88.7 MiB [] 66% 0.6s\u001b[0K\u001b[1G88.7 MiB [] 67% 0.6s\u001b[0K\u001b[1G88.7 MiB [] 68% 0.5s\u001b[0K\u001b[1G88.7 MiB [] 70% 0.5s\u001b[0K\u001b[1G88.7 MiB [] 71% 0.5s\u001b[0K\u001b[1G88.7 MiB [] 72% 0.5s\u001b[0K\u001b[1G88.7 MiB [] 73% 0.5s\u001b[0K\u001b[1G88.7 MiB [] 75% 0.4s\u001b[0K\u001b[1G88.7 MiB [] 76% 0.4s\u001b[0K\u001b[1G88.7 MiB [] 78% 0.4s\u001b[0K\u001b[1G88.7 MiB [] 79% 0.3s\u001b[0K\u001b[1G88.7 MiB [] 81% 0.3s\u001b[0K\u001b[1G88.7 MiB [] 82% 0.3s\u001b[0K\u001b[1G88.7 MiB [] 83% 0.3s\u001b[0K\u001b[1G88.7 MiB [] 84% 0.3s\u001b[0K\u001b[1G88.7 MiB [] 86% 0.2s\u001b[0K\u001b[1G88.7 MiB [] 88% 0.2s\u001b[0K\u001b[1G88.7 MiB [] 90% 0.2s\u001b[0K\u001b[1G88.7 MiB [] 91% 0.1s\u001b[0K\u001b[1G88.7 MiB [] 93% 0.1s\u001b[0K\u001b[1G88.7 MiB [] 96% 0.1s\u001b[0K\u001b[1G88.7 MiB [] 99% 0.0s\u001b[0K\u001b[1G88.7 MiB [] 100% 0.0s\u001b[0K\n",
            "Firefox 134.0 (playwright build v1471) downloaded to /root/.cache/ms-playwright/firefox-1471\n",
            "Downloading Webkit 18.2 (playwright build v2123)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/webkit/2123/webkit-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G89.2 MiB [] 0% 0.0s\u001b[0K\u001b[1G89.2 MiB [] 0% 40.2s\u001b[0K\u001b[1G89.2 MiB [] 0% 26.9s\u001b[0K\u001b[1G89.2 MiB [] 0% 19.8s\u001b[0K\u001b[1G89.2 MiB [] 0% 15.4s\u001b[0K\u001b[1G89.2 MiB [] 1% 7.2s\u001b[0K\u001b[1G89.2 MiB [] 2% 4.9s\u001b[0K\u001b[1G89.2 MiB [] 2% 4.1s\u001b[0K\u001b[1G89.2 MiB [] 3% 3.4s\u001b[0K\u001b[1G89.2 MiB [] 4% 3.2s\u001b[0K\u001b[1G89.2 MiB [] 5% 3.0s\u001b[0K\u001b[1G89.2 MiB [] 6% 2.8s\u001b[0K\u001b[1G89.2 MiB [] 6% 2.7s\u001b[0K\u001b[1G89.2 MiB [] 8% 2.5s\u001b[0K\u001b[1G89.2 MiB [] 8% 2.3s\u001b[0K\u001b[1G89.2 MiB [] 9% 2.2s\u001b[0K\u001b[1G89.2 MiB [] 11% 2.1s\u001b[0K\u001b[1G89.2 MiB [] 11% 2.3s\u001b[0K\u001b[1G89.2 MiB [] 12% 2.3s\u001b[0K\u001b[1G89.2 MiB [] 13% 2.3s\u001b[0K\u001b[1G89.2 MiB [] 14% 2.2s\u001b[0K\u001b[1G89.2 MiB [] 15% 2.1s\u001b[0K\u001b[1G89.2 MiB [] 16% 2.0s\u001b[0K\u001b[1G89.2 MiB [] 17% 2.0s\u001b[0K\u001b[1G89.2 MiB [] 18% 2.0s\u001b[0K\u001b[1G89.2 MiB [] 19% 1.9s\u001b[0K\u001b[1G89.2 MiB [] 20% 1.8s\u001b[0K\u001b[1G89.2 MiB [] 22% 1.7s\u001b[0K\u001b[1G89.2 MiB [] 23% 1.7s\u001b[0K\u001b[1G89.2 MiB [] 24% 1.6s\u001b[0K\u001b[1G89.2 MiB [] 25% 1.6s\u001b[0K\u001b[1G89.2 MiB [] 27% 1.5s\u001b[0K\u001b[1G89.2 MiB [] 28% 1.4s\u001b[0K\u001b[1G89.2 MiB [] 29% 1.4s\u001b[0K\u001b[1G89.2 MiB [] 31% 1.3s\u001b[0K\u001b[1G89.2 MiB [] 32% 1.3s\u001b[0K\u001b[1G89.2 MiB [] 34% 1.2s\u001b[0K\u001b[1G89.2 MiB [] 35% 1.2s\u001b[0K\u001b[1G89.2 MiB [] 37% 1.1s\u001b[0K\u001b[1G89.2 MiB [] 38% 1.1s\u001b[0K\u001b[1G89.2 MiB [] 39% 1.1s\u001b[0K\u001b[1G89.2 MiB [] 40% 1.0s\u001b[0K\u001b[1G89.2 MiB [] 41% 1.0s\u001b[0K\u001b[1G89.2 MiB [] 42% 1.0s\u001b[0K\u001b[1G89.2 MiB [] 43% 1.0s\u001b[0K\u001b[1G89.2 MiB [] 45% 0.9s\u001b[0K\u001b[1G89.2 MiB [] 46% 0.9s\u001b[0K\u001b[1G89.2 MiB [] 47% 0.9s\u001b[0K\u001b[1G89.2 MiB [] 49% 0.9s\u001b[0K\u001b[1G89.2 MiB [] 50% 0.8s\u001b[0K\u001b[1G89.2 MiB [] 51% 0.8s\u001b[0K\u001b[1G89.2 MiB [] 52% 0.8s\u001b[0K\u001b[1G89.2 MiB [] 53% 0.8s\u001b[0K\u001b[1G89.2 MiB [] 54% 0.8s\u001b[0K\u001b[1G89.2 MiB [] 56% 0.7s\u001b[0K\u001b[1G89.2 MiB [] 57% 0.7s\u001b[0K\u001b[1G89.2 MiB [] 59% 0.7s\u001b[0K\u001b[1G89.2 MiB [] 60% 0.6s\u001b[0K\u001b[1G89.2 MiB [] 62% 0.6s\u001b[0K\u001b[1G89.2 MiB [] 64% 0.6s\u001b[0K\u001b[1G89.2 MiB [] 65% 0.5s\u001b[0K\u001b[1G89.2 MiB [] 67% 0.5s\u001b[0K\u001b[1G89.2 MiB [] 68% 0.5s\u001b[0K\u001b[1G89.2 MiB [] 69% 0.5s\u001b[0K\u001b[1G89.2 MiB [] 71% 0.4s\u001b[0K\u001b[1G89.2 MiB [] 73% 0.4s\u001b[0K\u001b[1G89.2 MiB [] 74% 0.4s\u001b[0K\u001b[1G89.2 MiB [] 76% 0.3s\u001b[0K\u001b[1G89.2 MiB [] 78% 0.3s\u001b[0K\u001b[1G89.2 MiB [] 80% 0.3s\u001b[0K\u001b[1G89.2 MiB [] 82% 0.3s\u001b[0K\u001b[1G89.2 MiB [] 83% 0.2s\u001b[0K\u001b[1G89.2 MiB [] 84% 0.2s\u001b[0K\u001b[1G89.2 MiB [] 86% 0.2s\u001b[0K\u001b[1G89.2 MiB [] 88% 0.2s\u001b[0K\u001b[1G89.2 MiB [] 90% 0.1s\u001b[0K\u001b[1G89.2 MiB [] 92% 0.1s\u001b[0K\u001b[1G89.2 MiB [] 94% 0.1s\u001b[0K\u001b[1G89.2 MiB [] 96% 0.0s\u001b[0K\u001b[1G89.2 MiB [] 98% 0.0s\u001b[0K\u001b[1G89.2 MiB [] 99% 0.0s\u001b[0K\u001b[1G89.2 MiB [] 100% 0.0s\u001b[0K\n",
            "Webkit 18.2 (playwright build v2123) downloaded to /root/.cache/ms-playwright/webkit-2123\n",
            "Downloading FFMPEG playwright build v1011\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/ffmpeg/1011/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 2% 1.0s\u001b[0K\u001b[1G2.3 MiB [] 6% 0.6s\u001b[0K\u001b[1G2.3 MiB [] 14% 0.4s\u001b[0K\u001b[1G2.3 MiB [] 29% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 61% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1011 downloaded to /root/.cache/ms-playwright/ffmpeg-1011\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libgtk-4.so.1                                    ║\n",
            "║     libgraphene-1.0.so.0                             ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:865:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:963:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:952:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:122:7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3fd4af89-f557-4d19-b442-9047f09a1168",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fd4af89-f557-4d19-b442-9047f09a1168",
        "outputId": "bf918517-532e-402a-ea76-ad5d60d2680f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Future exception was never retrieved\n",
            "future: <Future finished exception=NetworkError('Protocol error Target.detachFromTarget: Target closed.')>\n",
            "pyppeteer.errors.NetworkError: Protocol error Target.detachFromTarget: Target closed.\n",
            "ERROR:asyncio:Future exception was never retrieved\n",
            "future: <Future finished exception=NetworkError('Protocol error (Target.sendMessageToTarget): No session with given id')>\n",
            "pyppeteer.errors.NetworkError: Protocol error (Target.sendMessageToTarget): No session with given id\n",
            "ERROR:asyncio:Future exception was never retrieved\n",
            "future: <Future finished exception=NetworkError('Protocol error Target.detachFromTarget: Target closed.')>\n",
            "pyppeteer.errors.NetworkError: Protocol error Target.detachFromTarget: Target closed.\n",
            "ERROR:asyncio:Future exception was never retrieved\n",
            "future: <Future finished exception=NetworkError('Protocol error (Target.sendMessageToTarget): No session with given id')>\n",
            "pyppeteer.errors.NetworkError: Protocol error (Target.sendMessageToTarget): No session with given id\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sitemap saved as sitemap_varanasi.xml\n",
            "✅ Scraping completed successfully!\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import nest_asyncio\n",
        "from requests_html import AsyncHTMLSession\n",
        "from urllib.parse import urljoin\n",
        "import xml.etree.ElementTree as ET\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# async def get_news_urls(base_url):\n",
        "#     urls = set()\n",
        "#     async with async_playwright() as p:\n",
        "#         browser = await p.chromium.launch(headless=True)\n",
        "#         page = await browser.new_page()\n",
        "#         await page.goto(base_url, timeout=60000)\n",
        "\n",
        "#         # Extract URLs\n",
        "#         links = await page.locator('a').all()\n",
        "#         for link in links:\n",
        "#             href = await link.get_attribute('href')\n",
        "#             if href and href.startswith(\"http\"):\n",
        "#                 urls.add(href)\n",
        "\n",
        "#         await browser.close()\n",
        "#     return urls\n",
        "\n",
        "# Fix event loop issue in Jupyter Notebook\n",
        "# nest_asyncio.apply()\n",
        "\n",
        "# Function to extract news URLs from a given page\n",
        "async def get_news_urls(base_url):\n",
        "    session = AsyncHTMLSession()\n",
        "    response = await session.get(base_url)\n",
        "    # await response.html.arender()  # Execute JavaScript\n",
        "    await response.html.arender(timeout=60, keep_page=True)\n",
        "    urls = set()\n",
        "    for link in response.html.find('a'):\n",
        "        href = link.attrs.get(\"href\", \"\")\n",
        "        if href.startswith(\"/\"):\n",
        "            url = urljoin(base_url, href)\n",
        "        else:\n",
        "            url = href\n",
        "\n",
        "        # Filter valid news links\n",
        "        if url.startswith(\"http\"):\n",
        "            urls.add(url)\n",
        "\n",
        "    return urls\n",
        "\n",
        "# Function to generate XML Sitemap\n",
        "def generate_sitemap(urls, output_file):\n",
        "    urlset = ET.Element(\"urlset\", xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\")\n",
        "\n",
        "    for url in urls:\n",
        "        url_element = ET.Element(\"url\")\n",
        "        loc_element = ET.Element(\"loc\")\n",
        "        loc_element.text = url\n",
        "        url_element.append(loc_element)\n",
        "        urlset.append(url_element)\n",
        "\n",
        "    tree = ET.ElementTree(urlset)\n",
        "    tree.write(output_file, encoding=\"utf-8\", xml_declaration=True)\n",
        "    print(f\"Sitemap saved as {output_file}\")\n",
        "\n",
        "# Main Execution (Handling Async)\n",
        "async def main(city):\n",
        "    city = city.lower().replace(\" \", \"-\")\n",
        "    all_urls = set()\n",
        "\n",
        "    # Scraping Times of India (Pages 1 to 5)\n",
        "    for page_num in range(1, 6):\n",
        "        toi_url = f\"https://timesofindia.indiatimes.com/city/{city}/{page_num}\"\n",
        "        urls = await get_news_urls(toi_url)\n",
        "        all_urls.update(urls)\n",
        "\n",
        "    # Scraping India TV News (Pages 6 to 10)\n",
        "    for page_num in range(6, 11):\n",
        "        indiatv_url = f\"https://www.indiatvnews.com/topic/{city}/{page_num}\"\n",
        "        urls = await get_news_urls(indiatv_url)\n",
        "        all_urls.update(urls)\n",
        "\n",
        "    # Save all URLs in a sitemap\n",
        "    generate_sitemap(all_urls, f\"sitemap_{city}.xml\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  ######## Change city_name here to scrape articles of a particular city ########\n",
        "    city_input = \"varanasi\"\n",
        "  ################################################################################\n",
        "    asyncio.run(main(city_input))\n",
        "    print(\"✅ Scraping completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "afd41c3e-8edc-4d4e-b1ef-70f4b1c3a3f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afd41c3e-8edc-4d4e-b1ef-70f4b1c3a3f7",
        "outputId": "ba60f023-84a5-4328-aadc-42e7f1abcfe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cleaned sitemap saved to: sitemap_varanasi_cleaned.xml\n"
          ]
        }
      ],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "\n",
        "\n",
        "def clean_sitemap(city_input):\n",
        "    \"\"\"Keep only article URLs of the desired format from TOI and India TV.\"\"\"\n",
        "    try:\n",
        "        # File names\n",
        "        input_file = f\"sitemap_{city_input}.xml\"\n",
        "        output_file = f\"sitemap_{city_input}_cleaned.xml\"\n",
        "\n",
        "        # Load sitemap\n",
        "        tree = ET.parse(input_file)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Namespace\n",
        "        namespace = {\"ns\": \"http://www.sitemaps.org/schemas/sitemap/0.9\"}\n",
        "\n",
        "        # Regex patterns (New formats)\n",
        "        toi_article_pattern = re.compile(rf\"https://timesofindia\\.indiatimes\\.com/city/{city_input}/.+/articleshow/\\d+\\.cms$\")\n",
        "        indiatv_article_pattern = re.compile(rf\"https://www\\.indiatvnews\\.com/{city_input}/.+-\\d{{4}}-\\d{{2}}-\\d{{2}}-\\d+$\")\n",
        "\n",
        "        # Filter URLs\n",
        "        for url_element in root.findall(\"ns:url\", namespace):\n",
        "            loc_element = url_element.find(\"ns:loc\", namespace)\n",
        "            if loc_element is not None:\n",
        "                url = loc_element.text\n",
        "                # Keep only valid article URLs\n",
        "                if not (toi_article_pattern.match(url) or indiatv_article_pattern.match(url)):\n",
        "                    root.remove(url_element)\n",
        "\n",
        "        # Save cleaned sitemap\n",
        "        tree.write(output_file, encoding=\"utf-8\", xml_declaration=True)\n",
        "        print(f\"✅ Cleaned sitemap saved to: {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error processing sitemap: {e}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "# city_input = \"delhi\"\n",
        "clean_sitemap(city_input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "44366140-7fd4-43f6-9886-cae915918090",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44366140-7fd4-43f6-9886-cae915918090",
        "outputId": "e7815015-f1c7-4052-e858-2a2f99876c36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔗 Found 59 URLs in sitemap.\n",
            "📰 Scraping 1/59: https://timesofindia.indiatimes.com/city/varanasi/pipraich-sugar-mill-to-begin-ethanol-prodn-with-90cr-allocation/articleshow/118429520.cms\n",
            "✅ Saved: Pipraich sugar mill to begin ethanol prodn with 90cr allocation\n",
            "📰 Scraping 2/59: https://timesofindia.indiatimes.com/city/varanasi/2-year-jail-to-ex-mla-5-aides-for-threatening-former-mps-nephew/articleshow/118487964.cms\n",
            "✅ Saved: 2-year jail to ex-MLA, 5 aides for threatening former MP’s nephew\n",
            "📰 Scraping 3/59: https://timesofindia.indiatimes.com/city/varanasi/govt-allocates-50cr-for-north-indias-first-forestry-varsity/articleshow/118429514.cms\n",
            "✅ Saved: Govt allocates 50cr for North India’s first forestry varsity\n",
            "📰 Scraping 4/59: https://timesofindia.indiatimes.com/city/varanasi/amidst-influx-of-pilgrims-naga-sadhus-continue-to-reach-kashi/articleshow/118366809.cms\n",
            "✅ Saved: Amidst influx of pilgrims, Naga Sadhus continue to reach Kashi\n",
            "📰 Scraping 5/59: https://timesofindia.indiatimes.com/city/varanasi/cultural-confluence-on-display-in-city-as-kts-5th-batch-from-tn-reaches-kashi/articleshow/118429484.cms\n",
            "✅ Saved: Cultural confluence on display in city as KTS 5th batch from TN reaches Kashi\n",
            "📰 Scraping 6/59: https://timesofindia.indiatimes.com/city/varanasi/achieving-development-goals-not-possible-sans-empowering-women-tn-guv-in-kashi/articleshow/118461420.cms\n",
            "✅ Saved: Achieving development goals not possible sans empowering women: TN Guv in Kashi\n",
            "📰 Scraping 7/59: https://timesofindia.indiatimes.com/city/varanasi/self-help-group-from-tn-explores-kashis-heritage/articleshow/118461411.cms\n",
            "✅ Saved: Self-help group from TN explores Kashi’s heritage\n",
            "📰 Scraping 8/59: https://timesofindia.indiatimes.com/city/varanasi/sitharaman-dedicates-375thelectric-locomotive-to-nation/articleshow/118456702.cms\n",
            "✅ Saved: Sitharaman dedicates 375thelectric locomotive to nation\n",
            "📰 Scraping 9/59: https://timesofindia.indiatimes.com/city/varanasi/3-labourers-dead-in-hit-and-run-incident-in-ups-mirzapur/articleshow/118503128.cms\n",
            "✅ Saved: Three Labourers Killed in Mirzapur Hit-and-Run Incident\n",
            "📰 Scraping 10/59: https://timesofindia.indiatimes.com/city/varanasi/ex-sp-mlcs-anticipatory-bail-plea-nixed/articleshow/118397053.cms\n",
            "✅ Saved: Ex-SP MLC’s anticipatory bail plea nixed\n",
            "📰 Scraping 11/59: https://timesofindia.indiatimes.com/city/varanasi/row-ends-as-martyr-name-back-on-board-of-school/articleshow/118366907.cms\n",
            "✅ Saved: Abdul Hamid Name Removed: 1965 war hero name removed from UP school board, restored after outcry\n",
            "📰 Scraping 12/59: https://timesofindia.indiatimes.com/city/varanasi/iitbhu-hosts-workshop-on-hindi-e-tools-and-official-language-implementation/articleshow/118530651.cms\n",
            "✅ Saved: IIT(BHU) hosts workshop on Hindi e-tools and official language implementation\n",
            "📰 Scraping 13/59: https://timesofindia.indiatimes.com/city/varanasi/3-nepalese-devotees-killed-in-scorpio-suv-collision-in-gorakhpur/articleshow/118488020.cms\n",
            "✅ Saved: 3 Nepalese devotees killed in Scorpio-SUV collision in Gorakhpur\n",
            "📰 Scraping 14/59: https://timesofindia.indiatimes.com/city/varanasi/police-extend-traffic-diversions-till-mahashivratri-as-kumbh-crowd-continues-in-city/articleshow/118429513.cms\n",
            "✅ Saved: Police extend traffic diversions till Mahashivratri as Kumbh crowd continues in city\n",
            "📰 Scraping 15/59: https://timesofindia.indiatimes.com/city/varanasi/as-lakhs-flock-to-kvt-admin-gears-up-for-mahashivratri-temple-releases-aarti-timings/articleshow/118511053.cms\n",
            "✅ Saved: As lakhs flock to KVT, admin gears up for Mahashivratri; temple releases aarti timings\n",
            "📰 Scraping 16/59: https://timesofindia.indiatimes.com/city/varanasi/kts-entrepreneurs-exchange-insights-for-shared-prosperity/articleshow/118396952.cms\n",
            "✅ Saved: KTS: Entrepreneurs exchange insights for shared prosperity\n",
            "📰 Scraping 17/59: https://timesofindia.indiatimes.com/city/varanasi/2cr-pilgrims-have-visited-kvt-since-start-of-maha-kumbh/articleshow/118397014.cms\n",
            "✅ Saved: 2cr pilgrims have visited KVT since start of Maha Kumbh\n",
            "📰 Scraping 18/59: https://timesofindia.indiatimes.com/city/varanasi/gda-orders-demolition-of-mosque/articleshow/118397089.cms\n",
            "✅ Saved: GDA orders demolition of mosque\n",
            "📰 Scraping 19/59: https://timesofindia.indiatimes.com/city/varanasi/21-day-caft-program-on-food-dairy-waste-valorisation-begins/articleshow/118366601.cms\n",
            "✅ Saved: 21-day CAFT program on food, dairy waste valorisation begins\n",
            "📰 Scraping 20/59: https://timesofindia.indiatimes.com/city/varanasi/oppn-seeking-political-gain-in-maha-kumbh-tragedy-bjp/articleshow/118511161.cms\n",
            "✅ Saved: Oppn seeking political gain in Maha Kumbh tragedy: BJP\n",
            "📰 Scraping 21/59: https://timesofindia.indiatimes.com/city/varanasi/jaishankar-highlights-indias-ai-potential-and-global-role/articleshow/118511184.cms\n",
            "✅ Saved: Jaishankar highlights India’s AI potential and global role\n",
            "📰 Scraping 22/59: https://timesofindia.indiatimes.com/city/varanasi/icmr-nhm-join-hands-to-reduce-infant-mortalityin-varanasi/articleshow/118396933.cms\n",
            "✅ Saved: ICMR, NHM join hands to reduce infant mortalityin Varanasi\n",
            "📰 Scraping 23/59: https://timesofindia.indiatimes.com/city/varanasi/intl-symposium-on-maha-kumbh-kicks-off/articleshow/118487966.cms\n",
            "✅ Saved: Int’l symposium on Maha Kumbh kicks off\n",
            "📰 Scraping 24/59: https://timesofindia.indiatimes.com/city/varanasi/8-dead-several-injured-in-two-separate-road-accidents-in-uttar-pradesh/articleshow/118404165.cms\n",
            "✅ Saved: Jaunpur Accident: 8 dead, several injured in two separate road accidents in Uttar Pradesh\n",
            "📰 Scraping 25/59: https://timesofindia.indiatimes.com/city/varanasi/grand-procession-of-akharas-naga-sadhusin-kashi-on-mahashivratri/articleshow/118396939.cms\n",
            "✅ Saved: Grand procession of akharas , naga sadhusin Kashi on Mahashivratri\n",
            "📰 Scraping 26/59: https://timesofindia.indiatimes.com/city/varanasi/kv-temples-garbhgrih-to-remain-open-for-45-hrs-for-mahashivratri/articleshow/118429524.cms\n",
            "✅ Saved: KV temple’s garbhgrih to remain open for 45 hrs for Mahashivratri\n",
            "📰 Scraping 27/59: https://timesofindia.indiatimes.com/city/varanasi/budget-to-propel-states-growth-mp/articleshow/118429510.cms\n",
            "✅ Saved: Budget to propel state’s growth: MP\n",
            "📰 Scraping 28/59: https://timesofindia.indiatimes.com/city/varanasi/health-camp-organised-for-inmates-at-gorakhpur-district-jail/articleshow/118511245.cms\n",
            "✅ Saved: Health camp organised for inmates at Gorakhpur district jail\n",
            "📰 Scraping 29/59: https://timesofindia.indiatimes.com/city/varanasi/sudhanshu-trivedi-highlights-union-budget-2025-26-as-a-catalyst-for-a-self-reliant-and-developed-india-/articleshow/118521698.cms\n",
            "✅ Saved: Sudhanshu Trivedi highlights union budget 2025-26 as a catalyst for self-reliant and developed India\n",
            "📰 Scraping 30/59: https://timesofindia.indiatimes.com/city/varanasi/iit-bhu-team-shines-at-shell-eco-marathon/articleshow/118396919.cms\n",
            "✅ Saved: IIT-BHU team shines at Shell Eco-Marathon\n",
            "📰 Scraping 31/59: https://timesofindia.indiatimes.com/city/varanasi/bhu-neurologists-invite-nadda-for-neuro-meet/articleshow/118461314.cms\n",
            "✅ Saved: BHU neurologists invite Nadda for Neuro meet\n",
            "📰 Scraping 32/59: https://timesofindia.indiatimes.com/city/varanasi/aiims-like-facilities-for-ims-bhu-nadda/articleshow/118461333.cms\n",
            "✅ Saved: AIIMS-like facilities for IMS-BHU: Nadda\n",
            "📰 Scraping 33/59: https://timesofindia.indiatimes.com/city/varanasi/kashi-tamil-sangamam-delegates-visit-hanuman-ghat-explore-cultural-and-spiritual-ties/articleshow/118529956.cms\n",
            "✅ Saved: Kashi Tamil Sangamam: Cultural and Spiritual Journeys at Hanuman Ghat\n",
            "📰 Scraping 34/59: https://timesofindia.indiatimes.com/city/varanasi/kumbh-crowd-mgmt-plan-revised-for-mahashivratri/articleshow/118397063.cms\n",
            "✅ Saved: Kumbh crowd mgmt plan revised for Mahashivratri\n",
            "📰 Scraping 35/59: https://timesofindia.indiatimes.com/city/varanasi/hit-run-three-labourers-killed-in-mirzapur/articleshow/118507827.cms\n",
            "✅ Saved: Hit & Run: Three labourers killed in Mirzapur\n",
            "📰 Scraping 36/59: https://timesofindia.indiatimes.com/city/varanasi/training-for-newly-appointed-jr-assistants/articleshow/118366691.cms\n",
            "✅ Saved: Training for newly appointed jr assistants\n",
            "📰 Scraping 37/59: https://timesofindia.indiatimes.com/city/varanasi/naga-guv-praises-exhibition-on-developed-india-at-kts/articleshow/118511110.cms\n",
            "✅ Saved: Naga Guv praises exhibition on ‘Developed India’ at KTS\n",
            "📰 Scraping 38/59: https://timesofindia.indiatimes.com/city/varanasi/52-year-old-neighbour-rapes-girl-in-ups-deoria/articleshow/118470307.cms\n",
            "✅ Saved: 52-year-old neighbour rapes girl in UP's Deoria\n",
            "📰 Scraping 39/59: https://timesofindia.indiatimes.com/city/varanasi/education-key-to-realising-vision-of-viksit-bharat/articleshow/118366704.cms\n",
            "✅ Saved: ‘Education key to realising vision of Viksit Bharat’\n",
            "📰 Scraping 40/59: https://timesofindia.indiatimes.com/city/varanasi/2-killed-5-injured-in-car-crash-in-ballia/articleshow/118481957.cms\n",
            "✅ Saved: 2 killed, 5 injured in car crash in Ballia\n",
            "📰 Scraping 41/59: https://timesofindia.indiatimes.com/city/varanasi/ap-govt-fetes-gi-man-rajani-kant/articleshow/118488031.cms\n",
            "✅ Saved: AP govt fetes ‘GI Man’ Rajani Kant\n",
            "📰 Scraping 42/59: https://timesofindia.indiatimes.com/city/varanasi/kashi-tamil-sangamam-celebrates-cultural-unity-in-varanasi/articleshow/118446487.cms\n",
            "✅ Saved: Kashi Tamil Sangamam: A Celebration of India's Cultural Unity in Varanasi\n",
            "📰 Scraping 43/59: https://timesofindia.indiatimes.com/city/varanasi/hindi-litt-for-kids-translated-into-tamil-at-workshop/articleshow/118487900.cms\n",
            "✅ Saved: Hindi litt for kids translated into Tamil at workshop\n",
            "📰 Scraping 44/59: https://timesofindia.indiatimes.com/city/varanasi/sangamam-celebration-of-age-old-connect-between-kashi-tamil-nadu-jaishankar/articleshow/118511240.cms\n",
            "✅ Saved: Sangamam celebration of age-old connect between Kashi, Tamil Nadu: Jaishankar\n",
            "📰 Scraping 45/59: https://timesofindia.indiatimes.com/city/varanasi/students-showcase-talent-at-kts-3-0/articleshow/118511219.cms\n",
            "✅ Saved: Students showcase talent at KTS 3.0\n",
            "📰 Scraping 46/59: https://timesofindia.indiatimes.com/city/varanasi/intl-meet-on-maha-kumbh-at-mppc-ends/articleshow/118511292.cms\n",
            "✅ Saved: Int’l meet on Maha Kumbh at MPPC ends\n",
            "📰 Scraping 47/59: https://timesofindia.indiatimes.com/city/varanasi/reed-sahib-dharamshala-basant-sarai-now-protected-monuments/articleshow/118397054.cms\n",
            "✅ Saved: Reed Sahib Dharamshala, Basant Sarai now protected monuments\n",
            "📰 Scraping 48/59: https://timesofindia.indiatimes.com/city/varanasi/academic-industry-meet-at-siddharth-univ-concludes/articleshow/118364259.cms\n",
            "✅ Saved: Academic-industry meet at Siddharth Univ concludes\n",
            "📰 Scraping 49/59: https://timesofindia.indiatimes.com/city/varanasi/experts-explore-connection-between-language-culture/articleshow/118461388.cms\n",
            "✅ Saved: Experts explore connection between language, culture\n",
            "📰 Scraping 50/59: https://timesofindia.indiatimes.com/city/varanasi/road-accidents-claim-lives-of-9-pilgrims-in-kashi-ghazipur/articleshow/118461434.cms\n",
            "✅ Saved: Road accidents claim lives of 9 pilgrims in Kashi, Ghazipur\n",
            "📰 Scraping 51/59: https://timesofindia.indiatimes.com/city/varanasi/ddu-gorakhpur-univ-unveils-literary-wall/articleshow/118364264.cms\n",
            "✅ Saved: DDU Gorakhpur Univ unveils ‘Literary Wall’\n",
            "📰 Scraping 52/59: https://timesofindia.indiatimes.com/city/varanasi/kvt-begins-mahashivratri-prep-amidst-kumbh-surge/articleshow/118487984.cms\n",
            "✅ Saved: KVT begins Mahashivratri prep amidst Kumbh surge\n",
            "📰 Scraping 53/59: https://timesofindia.indiatimes.com/city/varanasi/panic-after-bomb-hoax-in-kashi-exp-and-kamayani-exp-trains/articleshow/118364253.cms\n",
            "✅ Saved: Panic after bomb hoax in Kashi Exp and Kamayani Exp trains\n",
            "📰 Scraping 54/59: https://timesofindia.indiatimes.com/city/varanasi/arunachal-pradesh-honors-gi-man-of-india-rajani-kant-for-promoting-indigenous-products/articleshow/118473934.cms\n",
            "✅ Saved: Arunachal Pradesh Celebrates 'GI Man of India' Rajani Kant for Promoting Indigenous Products\n",
            "📰 Scraping 55/59: https://timesofindia.indiatimes.com/city/varanasi/10-pilgrims-dead-35-injured-in-three-separate-accidents/articleshow/118429519.cms\n",
            "✅ Saved: 10 pilgrims dead, 35 injured in three separate accidents\n",
            "📰 Scraping 56/59: https://timesofindia.indiatimes.com/city/varanasi/madani-mosque-litigant-dies-of-heart-attack/articleshow/118461315.cms\n",
            "✅ Saved: Madani mosque litigant dies of heart attack\n",
            "📰 Scraping 57/59: https://timesofindia.indiatimes.com/city/varanasi/bhu-exhibition-showcases-research-innovation-kashis-cultural-legacy/articleshow/118487971.cms\n",
            "✅ Saved: BHU exhibition showcases research, innovation & Kashi’s cultural legacy\n",
            "📰 Scraping 58/59: https://timesofindia.indiatimes.com/city/varanasi/malaviya-bhabha-hosps-treated-over-1l-cancer-patients-in-6yrs/articleshow/118487979.cms\n",
            "✅ Saved: Malaviya & Bhabha hosps treated over 1L cancer patients in 6yrs\n",
            "📰 Scraping 59/59: https://timesofindia.indiatimes.com/city/varanasi/naga-sadhus-to-enter-kvt-through-gate-4-for-post-kumbh-offerings/articleshow/118429515.cms\n",
            "✅ Saved: Naga sadhus to enter KVT through gate 4 for post Kumbh offerings\n",
            "\n",
            "✅ Saved 59 articles in MongoDB.\n"
          ]
        }
      ],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from newspaper import Article\n",
        "from pymongo import MongoClient\n",
        "from pymongo.errors import DuplicateKeyError\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "# MongoDB Connection\n",
        "MONGO_URI = \"mongodb+srv://anshikajyotijain:Saloni%401234@cluster0.u5jtr.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "client = MongoClient(MONGO_URI, tls=True, tlsAllowInvalidCertificates=True)\n",
        "db = client[\"news_database\"]\n",
        "collection = db[\"news_article\"]\n",
        "\n",
        "# Ensure URL uniqueness using an index\n",
        "# collection.create_index(\"url\", unique=True)\n",
        "\n",
        "\n",
        "def get_urls_from_sitemap(sitemap_file):\n",
        "    \"\"\"Parse XML sitemap and return a list of URLs.\"\"\"\n",
        "    try:\n",
        "        tree = ET.parse(sitemap_file)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        namespace = {\"ns\": \"http://www.sitemaps.org/schemas/sitemap/0.9\"}\n",
        "        urls = [url_element.find(\"ns:loc\", namespace).text for url_element in root.findall(\"ns:url\", namespace)]\n",
        "\n",
        "        return urls\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error reading sitemap: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def scrape_article(url):\n",
        "    \"\"\"Scrape article using newspaper3k.\"\"\"\n",
        "    try:\n",
        "        article = Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "\n",
        "        return {\n",
        "            \"url\": url,\n",
        "            \"title\": article.title,\n",
        "            \"text\": article.text,\n",
        "            \"top_image\": article.top_image,\n",
        "            \"publish_date\": article.publish_date.isoformat() if article.publish_date else None,\n",
        "            \"scraped_at\": datetime.datetime.now().isoformat()\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error scraping {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def main(city_input):\n",
        "    \"\"\"Scrape and save articles from sitemap_{city_input}_cleaned.xml.\"\"\"\n",
        "    sitemap_file = f\"sitemap_{city_input}_cleaned.xml\"\n",
        "    news_urls = get_urls_from_sitemap(sitemap_file)\n",
        "\n",
        "    if not news_urls:\n",
        "        print(\"⚠️ No URLs found in sitemap.\")\n",
        "        return\n",
        "\n",
        "    print(f\"🔗 Found {len(news_urls)} URLs in sitemap.\")\n",
        "\n",
        "    # Scrape articles and save them to MongoDB\n",
        "    scraped_articles = []\n",
        "    for i, url in enumerate(news_urls):\n",
        "        print(f\"📰 Scraping {i + 1}/{len(news_urls)}: {url}\")\n",
        "        article_data = scrape_article(url)\n",
        "\n",
        "        if article_data:\n",
        "            try:\n",
        "                collection.insert_one(article_data)  # Save to MongoDB\n",
        "                scraped_articles.append(article_data)\n",
        "                print(f\"✅ Saved: {article_data['title']}\")\n",
        "            except DuplicateKeyError:\n",
        "                print(f\"⚠️ Article already exists: {url}\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error saving to MongoDB: {e}\")\n",
        "\n",
        "        time.sleep(1)  # To avoid getting blocked\n",
        "\n",
        "    print(f\"\\n✅ Saved {len(scraped_articles)} articles in MongoDB.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # city_input = \"delhi\"\n",
        "    main(city_input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b10ae10e-c9b4-4e3a-889b-1cd069e6bf33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "b10ae10e-c9b4-4e3a-889b-1cd069e6bf33",
        "outputId": "7e9ee407-29ae-4113-ff01-281d6035d548"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-66b3cba88db2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# summary_collection.delete_many({})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load summarizer (BART)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msummarizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"summarization\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"facebook/bart-large-cnn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Load translation model (mBART)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0mmodel_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m         framework, model = infer_framework_load_model(\n\u001b[0m\u001b[1;32m    941\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0mmodel_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4243\u001b[0m                     \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4244\u001b[0m                     \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4245\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4246\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4247\u001b[0m                     \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4620\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4621\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4622\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4624\u001b[0m         \u001b[0;31m# Set some modules to fp32 if any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \"\"\"\n\u001b[1;32m   1028\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \"\"\"\n\u001b[1;32m   1028\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_initialize_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_is_hf_initialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hf_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36m_init_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    758\u001b[0m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
        "# from pymongo import MongoClient\n",
        "from pymongo.errors import DuplicateKeyError\n",
        "\n",
        "# MONGO_URI = \"mongodb+srv://anshikajyotijain:Saloni%401234@cluster0.u5jtr.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "# client = MongoClient(MONGO_URI, tls=True, tlsAllowInvalidCertificates=True)\n",
        "# db = client[\"news_database\"]\n",
        "# collection = db[\"articles\"]\n",
        "summary_collection = db[\"summaries\"]  # Target collection\n",
        "# summary_collection.delete_many({})\n",
        "# Load summarizer (BART)\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Load translation model (mBART)\n",
        "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Load zero-shot classifier\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Categories\n",
        "categories = [\"Politics\", \"Sports\", \"Business\", \"Entertainment\", \"Technology\"]\n",
        "\n",
        "\n",
        "# Step 1: Summarize text\n",
        "def summarize_text(text, max_length=130, min_length=70):\n",
        "    \"\"\"Summarize text using BART model.\"\"\"\n",
        "    try:\n",
        "        if len(text.split()) < 70:  # Skip summarization if too short\n",
        "            return text\n",
        "        summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
        "        return summary[0]['summary_text']\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error summarizing text: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Step 2: Translate text (English to Hindi)\n",
        "def translate_text(text, src_lang=\"en_XX\", tgt_lang=\"hi_IN\"):\n",
        "    \"\"\"Translate text using mBART model.\"\"\"\n",
        "    try:\n",
        "        tokenizer.src_lang = src_lang\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        outputs = model.generate(**inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang))\n",
        "        translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return translated_text\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error translating text: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Step 3: Classify article\n",
        "def classify_article(text):\n",
        "    \"\"\"Classify article using zero-shot classification into one of five categories.\"\"\"\n",
        "    try:\n",
        "        result = classifier(text, categories, multi_label=False)\n",
        "        return result[\"labels\"][0]  # Return the category with the highest score\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error classifying text: {e}\")\n",
        "        return \"Uncategorized\"\n",
        "\n",
        "\n",
        "# Step 4: Process and save articles\n",
        "def process_and_save_articles():\n",
        "    \"\"\"Fetch, summarize, translate, classify, and save articles.\"\"\"\n",
        "    try:\n",
        "        articles = collection.find({})\n",
        "        total_articles = collection.count_documents({})\n",
        "        processed_count = 0\n",
        "\n",
        "        for article in articles:\n",
        "            article_id = article[\"_id\"]\n",
        "            text = article.get(\"text\", \"\")\n",
        "            title = article.get(\"title\", \"\")\n",
        "            url = article.get(\"url\", \"\")\n",
        "            image = article.get(\"image\", None)\n",
        "\n",
        "            if text:\n",
        "                summary = summarize_text(text)\n",
        "                # translated_summary = translate_text(summary) if summary else None\n",
        "                category = classify_article(text)\n",
        "\n",
        "                if summary:\n",
        "                    document = {\n",
        "                        \"_id\": article_id,\n",
        "                        \"title\": title,\n",
        "                        # \"url\": url,\n",
        "                        \"category\": category,\n",
        "                        \"summary\": summary,\n",
        "                        # \"translated_summary\": translated_summary\n",
        "                    }\n",
        "\n",
        "                    if image:\n",
        "                        document[\"image\"] = image\n",
        "\n",
        "                    try:\n",
        "                        summary_collection.insert_one(document)\n",
        "                        processed_count += 1\n",
        "                        print(f\"✅ Processed {processed_count}/{total_articles} articles. Category: {category}\")\n",
        "                    except DuplicateKeyError:\n",
        "                        print(f\"⚠️ Article already exists: {url}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error processing articles: {e}\")\n",
        "\n",
        "\n",
        "# Step 5: Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    process_and_save_articles()\n",
        "    print(\"✅ All articles processed, categorized, and saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mBCey8oDhJH5"
      },
      "id": "mBCey8oDhJH5",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}